<html>
<head>
    <meta charset="UTF-8">
    <title>Binary data types</title>
    <link rel="stylesheet" type="text/css" media="all" href="styles.css" />
    <link href="https://fonts.googleapis.com/css?family=Unica+One|Noticia+Text:400,400i,700,700i|Fira+Mono:400,700,&amp;subset=latin-ext" rel="stylesheet">
    <link rel="stylesheet" href="highlight/styles/github.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="highlight/highlight.pack.js"></script>
    <script>
        $(document).ready(function()
        {
          $('code.swift').each(function(i, block)
          {
            hljs.highlightBlock(block);
          });
        });
    </script>
</head>
<body>

    <a class="nav-triangle-right" href="chapter2.html"><div>Next</div></a>

    <div class="content-container">
        <div class="content">
            <div class="chapter-header"><div class="chapter-number">1</div><h1>Binary data types</h1></div><p>If you already know binary, perhaps if you’re coming from another low-level language like C, it’s not the end of the world if you skip this chapter. But I encourage you to review it anyway. Swift arguably cares more about binary representation than even C does.</p><h2>Binary means base two</h2><p><keyword>Binary</keyword> is just another name for <keyword>base two</keyword>. That’s why binary has two digits — 0 and 1. Our “normal” counting is done in <keyword>decimal</keyword>, or <keyword>base ten</keyword>. It has ten digits — 0 through 9.</p><p>We usually call <strong>b</strong>inary dig<strong>its</strong> <em><keyword>bits</keyword></em> for short. Bits are special because they only exist in two forms, 0 and 1. This makes them easy to represent with electronics — “on” means 1, “off” means 0.</p><h2>Data types give meaning to groups of bits</h2><p>Data types are specific ways to interpret an otherwise meaningless group of bits. When a binary value has an associated data type, we say that it is <keyword>bound</keyword> to that specific data type. That data type tells you how you should read the binary value, for example as a number, or a letter, or a true-false value, or as a day of the week, etc. You’ll see examples of specific Swift data types in the coming sections and chapters.</p><aside>When we get into <keyword>dynamic programming</keyword>, it’s possible for binary data to “tell” you what specific data type it is. But of course, you still have to know to ask it, and that requires knowing enough beforehand to assume that it’s some sort of dynamic data type in the first place.</aside><p>Bits have no inherent data type. You cannot look at a binary value and say, <inline-quote>well, this looks like a letter to me</inline-quote>. Data types are all in the compiler, and the programmer’s head. CPUs don’t know anything about data types, they operate on data <em>assuming</em> it’s bound to a certain data type, even if that binding doesn’t make any sense.</p><h2>Swift is a strongly typed programming language</h2><p>Swift is what’s known in the taxonomy of programming languages as a <keyword>strongly typed</keyword> language. This means that Swift forces you to be specific as to which binary values are bound to which types, and it forces you to be consistent when passing around and operating on those values in your program. This is in contrast to <keyword>weakly typed</keyword> languages, where “anything goes” and you’re allowed to (accidentally) change the data type of a binary value, changing its meaning.</p><aside>There is a Pythonista reading this who swears that Python is a strongly typed language. This is true — at run-time. Swift is strongly typed at <em>compile-time</em>. This means that in Swift, type bugs get caught while the program is being compiled, whereas in Python, they don’t get caught until they crash the program. Of course, this is still better than them never getting caught at all.</aside><p>Using a strongly typed language like Swift might seem like a lot of work. Programmers coming from weakly typed languages like Python, C, and C++ often complain about having to make data types match exactly in Swift. However strong typing can help prevent lots and lots of bugs, since you almost never want to implicitly change the interpretation of a data value. When you do, Swift forces you to be explicit about it. These consistency guarantees are collectively known as <keyword>type safety</keyword> and it’s one of the most fundamental features of Swift’s design.</p><h2>Bits can represent whole numbers</h2><p>Just like we can string together multiple decimal digits to represent numbers bigger than 9, we can string together multiple bits to represent numbers bigger than 1. Every base has <keyword>place values</keyword>. In decimal, the place values are ones, tens, hundreds, thousands, and so on. In binary, the place values are ones, twos, fours, eights, sixteens, and so on.</p><table class="place-table"><tr><td>Thousands</td> <td>Hundreds</td> <td>Tens</td> <td>Ones</td> <td>Total</td></tr>
<tr><td>1</td> <td>9</td> <td>8</td> <td>9</td> <td>= 1000 + 900 + 80 + 9</td></tr>
<tr><td colspan="4"></td> <td>= 1989</td></tr></table><table class="place-table"><tr><td>Eights</td> <td>Fours</td> <td>Twos</td> <td>Ones</td> <td>Total</td></tr>
<tr><td>1</td> <td>1</td> <td>0</td> <td>1</td> <td>= 8 + 4 + 1</td></tr>
<tr><td colspan="4"></td> <td>= 13</td></tr></table><p>Notice that the largest number you can count up to with <simple-math><var>n</var></simple-math> unsigned decimal digits is <simple-math>10<sup><var>n</var></sup> − 1</simple-math>. If you have three decimal digits, you can count up to <simple-math>10<sup>3</sup> − 1</simple-math>, or 999. Similarly, if you have <simple-math><var>n</var></simple-math> unsigned <em>binary</em> digits, you can count all the way up to <simple-math>2<sup><var>n</var></sup> − 1</simple-math>. So if you have eight bits, you can count all the way up to <simple-math>2<sup>8</sup> − 1</simple-math>, or 255.</p><aside>To be perfectly specific, 8 bits is an <keyword>octet</keyword>, which is <em>almost</em> always the same thing as a byte. The exceptions are too few to be within the scope of this book.</aside><p>8 bit sets are special because 8 bits is what we usually call a <keyword>byte</keyword>. Interpreting a byte as a number between <code class="swift">0</code> and <code class="swift">255</code> (inclusive) is an example of a data type. In Swift, we call it a <code class="swift">UInt8</code>, which is short for unsigned 8-bit integer. “<keyword>Unsigned</keyword>” means it’s always a positive number. “<keyword>Integer</keyword>” means it’s a whole number, and “8-bit” means, well, it’s 8 bits long.</p><h2>Integer addition can overflow</h2><p>Binary addition works a lot like decimal addition. You add up the bits in each place value from right to left, carrying the 1 when necessary.</p><div class="centered-container"><ascii-figure>  1 1<br/>  0 1 1 0 = 6<br/>+ 0 0 1 1 = 3<br/>—————————<br/>  1 0 0 1 = 9</ascii-figure></div><p>Since this is all implemented in hardware as a nanoscopic circuit inside the CPU, and circuits don’t really have a “right” or “left”, we’ll start calling the bit on the right the <keyword>least significant bit</keyword>, or <keyword>LSB</keyword>, and the bit on the left the <keyword>most significant bit</keyword>, or <keyword>MSB</keyword>. Significance increases from right to left because the place value gets bigger and bigger.</p><div class="centered-container"><ascii-figure>0 1 0 1 1 0 0 1<br/>more significant ←→ less significant</ascii-figure></div><p>Remember though, that we have a fixed number of bits. A <code class="swift">UInt8</code> is <em>always</em> 8 bits long, no more, no less. And sometimes, when adding two <code class="swift">UInt8</code>s, you have to carry a 1 off of the most significant bit.</p><div class="centered-container"><ascii-figure>  1 1 1 1 1 1 1 1<br/>    1 1 1 1 1 1 1 1 = 255<br/>+   0 0 0 0 0 0 0 1 =   1<br/>———————————————————<br/>  _ 0 0 0 0 0 0 0 0 =   0???</ascii-figure></div><aside>A few languages like Java, Ruby, and Python can automatically extend their integer types at run-time when they detect arithmetic overflow, a lot like how you would use extra space on the the paper to write down the carry-out digit when doing addition by hand. These integer types are called <keyword>bignums</keyword> or <keyword>big-integers</keyword>. These languages sacrifice performance so that you don’t have to worry about integer overflow. Swift makes the opposite tradeoff, the implication being that Swift programs are fast, but you have to be aware of integer overflow.</aside><p>That last 1 gets lost, effectively subtracting 256 from the result. That means that if you keep incrementing a <code class="swift">UInt8</code> value, eventually it will wrap around and start counting back up from zero. Think of an analog car odometer — once it hits 999,999, the next number it shows is 000,000. This phenomenon is called <keyword>integer overflow</keyword>.</p><h2>Integer overflow gives us subtraction</h2><p>You <em>could</em> try and do binary subtraction just like we did with binary addition.</p><div class="centered-container"><ascii-figure>             −1 10<br/>  0 0 0 1 0 0 1 0 = 18<br/>− 0 0 0 0 0 0 0 1 =  1<br/>—————————————————<br/>  0 0 0 1 0 0 0 1 = 17</ascii-figure></div><p>Subtraction is a lot harder than addition, mostly because you have to borrow, though in principle, you <em>could</em> build a CPU circuit that performs subtraction just like the one that performs addition. But it turns out there’s an easier way.</p><p>The easiest way to understand it is to imagine that you have space for three decimal digits — you can count from 0 all the way to 999. One way to subtract 1 from an arbitrary three-digit decimal number without <em>actually doing subtraction</em> is to <em>add</em> 999 to it. For example, if you had the number 25, adding 999 to it would give you 1024. But 1024 overflows the space for three digits, so you get rid of the 1 in the thousands place, leaving you with 24.</p><p>This works because adding 999 is the same as adding 1000 and subtracting 1. Integer overflow then subtracts 1000 from the total, so in the end you are effectively subtracting 1. Similarly, you can subtract 2 by adding 998, 3 by adding 997, and so forth.</p><div class="centered-container"><ascii-figure>   NNN + 999<br/>=  NNN + 1000 − 1<br/>= 1NNN − 1<br/>→  NNN - 1 </ascii-figure></div><div class="centered-container"><ascii-figure>   NNN + 998<br/>=  NNN + 1000 − 2<br/>= 1NNN − 2<br/>→  NNN - 2 </ascii-figure></div><div class="centered-container"><ascii-figure>   NNN + 997<br/>=  NNN + 1000 − 3<br/>= 1NNN − 3<br/>→  NNN - 3 </ascii-figure></div><p>In other words, to subtract a number <simple-math><var>n</var></simple-math>, you add <simple-math><var>n</var></simple-math> less than the <em>smallest number not representable</em> by the digits you have. Remember, this number is equal to <simple-math><var>base</var> <sup><var>digits</var></sup></simple-math>. So for a <code class="swift">UInt8</code>, you subtract 1 by adding 255. You subtract 2 by adding 254, and so forth. You can use addition to get subtraction “for free”.</p><h2>Subtraction gives us negative numbers</h2><aside>This should come naturally to any of you who know <keyword>modular arithmetic</keyword>. If you haven’t figured it out already, the <keyword>modulus</keyword> of a binary number with <simple-math><var>n</var></simple-math> bits is always <simple-math>2<sup><var>n</var></sup></simple-math>.</aside><p>Mind you that subtracting <simple-math><var>n</var></simple-math> is the same thing as adding <simple-math>−<var>n</var></simple-math>. And since we just established that subtracting <simple-math><var>n</var></simple-math> is also the same thing as adding <simple-math><var>base</var> <sup><var>digits</var></sup> − <var>n</var></simple-math>, that implies that <simple-math>−<var>n</var> = <var>base</var> <sup><var>digits</var></sup> − <var>n</var></simple-math>. Now we have <em>two</em> ways of interpreting any binary number. We can interpret the 8 bit number <code class="swift">0101,1001</code> as the positive number <simple-math>89</simple-math>, or we can interpret it as the negative number <simple-math>89 − 256 = −167</simple-math>.</p><h2>Negate numbers using the two’s complement</h2><aside>Don’t confuse the negative of a number with the <em>negative interpretation</em> of a number. The latter always has the exact same bit pattern as the number because it’s just another way of reading it. The negative of a number has a completely different bit pattern.</aside><p>Subtraction-by-addition isn’t very useful if you don’t have an easy way of negating a number. Let’s go back to the three-digit decimal example. To get the negative of a three-digit decimal number <simple-math><var>n</var></simple-math>, you have to compute <simple-math>1000 − <var>n</var></simple-math>. This quantity is called the <keyword>ten’s complement</keyword> of the number. But doing <simple-math>1000 − <var>n</var></simple-math> directly is hard because you have to borrow all the way out to the hundreds place. So instead, we do <simple-math>999 − <var>n</var></simple-math>, and then add 1 later.</p><div class="centered-container"><ascii-figure>  9 9 9<br/>− 7 8 9<br/>———————<br/>  2 1 0 = nines complement<br/>+     1<br/>———————<br/>  2 1 1 = ten’s complement</ascii-figure></div><aside>Don’t confuse the nines complement with the <em>nine’s</em> complement. (There’s an apostrophe.) The nines complement (without the apostrophe) is a digit-wise operation <simple-math>999 − <var>n</var></simple-math>. The nine’s complement (with the apostrophe) is equal to <simple-math>9<sup>3</sup> − <var>n</var> = 729 − <var>n</var></simple-math>, and is utterly irrelevant here.</aside><p>Notice that no matter what you subtract from 999, you <em>never</em> have to borrow. This means you can do the subtraction on each pair of digits without having to worry about any of the surrounding digits. We call the result of <simple-math>999 − <var>n</var></simple-math> the <keyword>nines complement</keyword> of <simple-math><var>n</var></simple-math>. Then we add 1 to get <simple-math>1000 − <var>n</var></simple-math>, but we already know how to add 1 to a number.</p><p>Doing it in binary is even easier because computing the <keyword>ones complement</keyword> is just a matter of flipping the bits. Adding 1 gives you the <keyword>two’s complement</keyword>, the negative of the number.</p><div class="centered-container"><ascii-figure>  1 1 1 1 1 1 1 1<br/>− 0 1 0 1 1 0 0 1<br/>—————————————————<br/>  1 0 1 0 0 1 1 0 = ones complement<br/>+               1<br/>—————————————————<br/>  1 0 1 0 0 1 1 1 = two’s complement</ascii-figure></div><h2>The most significant bit separates negative numbers from positive numbers</h2><aside>What about zero, you ask? Zero is signless because its complement in any base is always itself. Don’t believe me? Try it!</aside><p>Up until now, every 8 bit number has had two interpretations — a positive one, and a negative one. This is pretty silly, so we’ll define numbers with 1 as their most significant bit as negative, and numbers with 0 as their most significant bit as positive. For our purposes, zero rolls with the positives. We’ll make this a new data type — a <em>signed</em> 8 bit integer, or an <code class="swift">Int8</code>. (Because <code class="swift">SInt8</code> sounds stupid.) The smallest <code class="swift">Int8</code> is <code class="swift">1000,0000</code>, or −128. The largest <code class="swift">Int8</code> is <code class="swift">0111,1111</code>, or +127.</p><aside>Flipping the most significant bit of an <code class="swift">Int8</code> adds 128 to its numeric value, mapping the range <simple-math>[−128, 128) to [0, 256)</simple-math>. Do you understand why?</aside><p>Defining negatives and positives this way makes comparisons easy because you can compare any two <code class="swift">Int8</code>s by flipping their most significant bit, and then comparing them as if they were <code class="swift">UInt8</code>s.</p><p>Just like unsigned integers, signed integers can overflow. However, in terms of bit patterns, their overflow boundary occurs <a href="https://xkcd.com/571/" target="_blank">half a period out of phase</a> with that of unsigned integers.</p><div class="warning-container"><h2>Warning!</h2><p><warning>Unlike Swift, in some languages such as C and C++, signed integer overflow is undefined behavior.</warning></p></div><table class="data-table"><tr><td>Bit pattern</td> <td>Signed value</td> <td>Unsigned value</td> <td class="invisible"></td></tr>
<tr><td>0000,0000</td> <td>0</td> <td>0</td></tr>
<tr><td>0000,0001</td> <td>1</td> <td>1</td></tr>
<tr><td>0000,0010</td> <td>2</td> <td>2</td></tr>
<tr><td>0000,0011</td> <td>3</td> <td>3</td></tr>
<tr><td class="ellipsis"></td> <td class="ellipsis"></td> <td class="ellipsis"></td></tr>
<tr><td>0111,1110</td> <td>126</td> <td>126</td></tr>
<tr><td>0111,1111</td> <td>127</td> <td>127</td></tr>
<tr><td>1000,0000</td> <td><span style="background-color: rgba(255, 0, 50, 0.2);" class="highlight">−128</span></td> <td>128</td> <td class="annotation">← Signed overflow</td></tr>
<tr><td>1000,0001</td> <td>−127</td> <td>129</td></tr>
<tr><td>1000,0010</td> <td>−126</td> <td>130</td></tr>
<tr><td class="ellipsis"></td> <td class="ellipsis"></td> <td class="ellipsis"></td></tr>
<tr><td>1111,1101</td> <td>−3</td> <td>253</td></tr>
<tr><td>1111,1110</td> <td>−2</td> <td>254</td></tr>
<tr><td>1111,1111</td> <td>−1</td> <td>255</td></tr>
<tr><td>0000,0000</td> <td>0</td> <td><span style="background-color: rgba(255, 0, 50, 0.2);" class="highlight">0</span></td> <td class="annotation">← Unsigned overflow</td></tr></table>
        </div>
    </div>
</body>
</html>